{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "Это уже третий семинар по собственно NLP, ура! В предыдущих сериях мы занимались частотностью, N-граммами, а также лемматизацией и морфологической разметкой для русского языка. Пришло время для более сложных задач! Кстати, какие задачи NLP вы знаете?\n",
    "\n",
    "Существует несколько хороших NLP-библиотек для питона:\n",
    "* Natural Language Toolkit (NLTK)\n",
    "* Apache OpenNLP\n",
    "* Stanford NLP suite\n",
    "* Gate NLP library\n",
    "* Spacy\n",
    "\n",
    "Сегодня мы поработаем с самой первой и самой известной из них — NLTK. Устанавливается эта библиотека стандартно, командой `pip install nltk`. Но поскольку в эту библиотеку помимо скриптов входит еще много разных данных — текстовые корпуса, предобученные модели для анализа тональности и морфологической разметки, списки стоп-слов для разных языков и т.п. — перед началом работы понадобится скачать нужные данные. \n",
    "\n",
    "**NB!** Не нужно скачивать данные (писать `nltk.download()`) каждый раз после импорта!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В открывшемся окошке нужно выбрать и скачать следующие пакеты:\n",
    "\n",
    "1. Models\n",
    "    - punkt\n",
    "    - snowball_data\n",
    "    - porter_test\n",
    "    - maxent_ne_chunker *(необязательно сейчас, но на будущее пригодится)*\n",
    "    - maxent_treebank_pos *(на будущее)*\n",
    "    - averaged_perceptron *(на будущее)*\n",
    "    - averaged_perceptron_russian *(на будущее)*\n",
    "2. Corpora\n",
    "    - movie_reviews\n",
    "    - stopwords\n",
    "    - brown *(на будущее)*\n",
    "3. All Packages\n",
    "    - wordnet\n",
    "    - universal_tagset\n",
    "    \n",
    "Можно скачать и все данные сразу (`Collections > all`), но они будут качаться долго и займут много места. И среди них есть вещи, которые скорее всего вам не понадобятся -- например, баскский корпус.\n",
    "\n",
    "Мы посмотрим лишь на базовые функции NLTK и разберем один сложный пример. Для дальнейшей работы [вот тут](https://www.nltk.org/book/) можно найти учебник по NLTK от его авторов, а [вот тут](https://github.com/hb20007/hands-on-nltk-tutorial) много тьюториалов, где показывается, как решать различные задачи с помощью инструментов NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Военно-морская любовь\n",
    "\n",
    "По морям, играя, носится\n",
    "с миноносцем миноносица.\n",
    "\n",
    "Льнет, как будто к меду осочка,\n",
    "к миноносцу миноносочка.\n",
    "\n",
    "И конца б не довелось ему,\n",
    "благодушью миноносьему.\n",
    "\n",
    "Вдруг прожектор, вздев на нос очки,\n",
    "впился в спину миноносочки.\n",
    "\n",
    "Как взревет медноголосина:\n",
    "\"Р-р-р-астакая миноносина!\"\n",
    "\n",
    "Прямо ль, влево ль, вправо ль бросится,\n",
    "а сбежала миноносица.\n",
    "\n",
    "Но ударить удалось ему\n",
    "по ребру по миноносьему.\n",
    "\n",
    "Плач и вой морями носится:\n",
    "овдовела миноносица.\n",
    "\n",
    "И чего это несносен нам\n",
    "мир в семействе миноносином?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Военно-морская', 'любовь', 'По', 'морям', ',', 'играя', ',', 'носится', 'с', 'миноносцем', 'миноносица', '.', 'Льнет', ',', 'как', 'будто', 'к', 'меду', 'осочка', ',', 'к', 'миноносцу', 'миноносочка', '.', 'И', 'конца', 'б', 'не', 'довелось', 'ему', ',', 'благодушью', 'миноносьему', '.', 'Вдруг', 'прожектор', ',', 'вздев', 'на', 'нос', 'очки', ',', 'впился', 'в', 'спину', 'миноносочки', '.', 'Как', 'взревет', 'медноголосина', ':', \"''\", 'Р-р-р-астакая', 'миноносина', '!', \"''\", 'Прямо', 'ль', ',', 'влево', 'ль', ',', 'вправо', 'ль', 'бросится', ',', 'а', 'сбежала', 'миноносица', '.', 'Но', 'ударить', 'удалось', 'ему', 'по', 'ребру', 'по', 'миноносьему', '.', 'Плач', 'и', 'вой', 'морями', 'носится', ':', 'овдовела', 'миноносица', '.', 'И', 'чего', 'это', 'несносен', 'нам', 'мир', 'в', 'семействе', 'миноносином', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сплиттинг\n",
    "\n",
    "Этим красивым словом называется разбиение текста на предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nВоенно-морская любовь\\n\\nПо морям, играя, носится\\nс миноносцем миноносица.',\n",
       " 'Льнет, как будто к меду осочка,\\nк миноносцу миноносочка.',\n",
       " 'И конца б не довелось ему,\\nблагодушью миноносьему.',\n",
       " 'Вдруг прожектор, вздев на нос очки,\\nвпился в спину миноносочки.',\n",
       " 'Как взревет медноголосина:\\n\"Р-р-р-астакая миноносина!\"',\n",
       " 'Прямо ль, влево ль, вправо ль бросится,\\nа сбежала миноносица.',\n",
       " 'Но ударить удалось ему\\nпо ребру по миноносьему.',\n",
       " 'Плач и вой морями носится:\\nовдовела миноносица.',\n",
       " 'И чего это несносен нам\\nмир в семействе миноносином?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По результату работы сплиттера можно сделать вывод, что для разбиения на предложения важны сочетания знаков препинания и больших букв. Знак переноса строки, во-первых, никак не влияет на разбиение, а во-вторых, остается в тексте, как и любой другой символ. А что если мы хотим от него избавиться? Можно сделать это с помощью регулярных выражений, например."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Военно-морская любовь  По морям, играя, носится с миноносцем миноносица.', 'Льнет, как будто к меду осочка, к миноносцу миноносочка.', 'И конца б не довелось ему, благодушью миноносьему.', 'Вдруг прожектор, вздев на нос очки, впился в спину миноносочки.', 'Как взревет медноголосина: \"Р-р-р-астакая миноносина!\"', 'Прямо ль, влево ль, вправо ль бросится, а сбежала миноносица.', 'Но ударить удалось ему по ребру по миноносьему.', 'Плач и вой морями носится: овдовела миноносица.', 'И чего это несносен нам мир в семействе миноносином?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "clean_sents = [re.sub(r'\\n', r' ', sent) for sent in sent_tokenize(text)]\n",
    "print(clean_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова\n",
    "\n",
    "В предыдущем семинаре мы уже работали со стоп-словами — высокочастотными союзами, предлогами и другими служебными частями речи, которые не дают нам никакой информации о конкретном тексте. В NLTK есть готовые списки стоп-слов для следующих языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# смотрим, какие языки есть\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# загружаем нужный список стоп-слов\n",
    "sw = stopwords.words('russian')\n",
    "\n",
    "# смотрим, что внутри\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['любовь', 'морям', 'играя', 'носится', 'миноносцем', 'миноносица', 'льнет', 'меду', 'осочка', 'миноносцу', 'миноносочка', 'конца', 'б', 'довелось', 'благодушью', 'миноносьему', 'прожектор', 'вздев', 'нос', 'очки', 'впился', 'спину', 'миноносочки', 'взревет', 'медноголосина', 'миноносина', 'прямо', 'ль', 'влево', 'ль', 'вправо', 'ль', 'бросится', 'сбежала', 'миноносица', 'ударить', 'удалось', 'ребру', 'миноносьему', 'плач', 'вой', 'морями', 'носится', 'овдовела', 'миноносица', 'это', 'несносен', 'нам', 'мир', 'семействе', 'миноносином']\n"
     ]
    }
   ],
   "source": [
    "# токенизируем текст, приводим к нижнему регистру и оставляем только последовательности из букв,\n",
    "# т.е. все токены, где были знаки препинания и числа, исчезнут\n",
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "\n",
    "# какие слова исчезли?\n",
    "filtered = [w for w in words if w not in sw]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-граммы\n",
    "\n",
    "**N-граммы** — это сочетания из N элементов (слов, символов), идущих друг за другом. Одиночные элементы называются униграммами, сочетания из двух элементов — биграммами, из трёх — триграммами, а дальше все пишется цифрами: 4-граммы, 5-граммы и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('военно-морская', 'любовь'),\n",
       " ('любовь', 'по'),\n",
       " ('по', 'морям'),\n",
       " ('морям', ','),\n",
       " (',', 'играя'),\n",
       " ('играя', ','),\n",
       " (',', 'носится'),\n",
       " ('носится', 'с'),\n",
       " ('с', 'миноносцем'),\n",
       " ('миноносцем', 'миноносица'),\n",
       " ('миноносица', '.'),\n",
       " ('.', 'льнет'),\n",
       " ('льнет', ','),\n",
       " (',', 'как'),\n",
       " ('как', 'будто'),\n",
       " ('будто', 'к'),\n",
       " ('к', 'меду'),\n",
       " ('меду', 'осочка'),\n",
       " ('осочка', ','),\n",
       " (',', 'к')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b for b in nltk.bigrams(word_tokenize(text.lower()))][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object trigrams at 0x0000017D443DE3B8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# функции bigrams и trigrams выдают специальный объект \"генератор\",\n",
    "# по которому можно итерироваться\n",
    "nltk.trigrams(word_tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг\n",
    "\n",
    "**Стемминг** — отсечение от слова окончаний и суффиксов, чтобы оставшаяся часть, называемая stem, была одинаковой для всех грамматических форм слова. Стем необязательно совпадает с морфлогической основой слова. Одинаковый стем может получиться и не у однокоренных слов и наоборот (в этом проблема стемминга).\n",
    "\n",
    "### Стеммер Портера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by: by\n",
      "calamity: calam\n",
      "coil: coil\n",
      "come: come\n",
      "consummation: consumm\n",
      "death: death\n",
      "die: die\n",
      "dream: dream\n",
      "dreams: dream\n",
      "end: end\n",
      "flesh: flesh\n",
      "fortune: fortun\n",
      "give: give\n",
      "have: have\n",
      "heartache: heartach\n",
      "heir: heir\n",
      "in: in\n",
      "is: is\n",
      "life: life\n",
      "long: long\n",
      "makes: make\n",
      "may: may\n",
      "mind: mind\n",
      "more: more\n",
      "mortal: mortal\n",
      "natural: natur\n",
      "nobler: nobler\n",
      "not: not\n",
      "of: of\n",
      "off: off\n"
     ]
    }
   ],
   "source": [
    "# неловкий момент: работает только для английского\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "hamlet = \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them. To die — to sleep,\n",
    "No more; and by a sleep to say we end\n",
    "The heartache and the thousand natural shocks\n",
    "That flesh is heir to: 'tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep, perchance to dream — ay, there's the rub:\n",
    "For in that sleep of death what dreams may come,\n",
    "When we have shuffled off this mortal coil,\n",
    "Must give us pause — there's the respect\n",
    "That makes calamity of so long life.\n",
    "\"\"\"\n",
    "\n",
    "porter = PorterStemmer()\n",
    "words = set(word_tokenize(hamlet))\n",
    "\n",
    "for w in sorted(words)[25:55]:\n",
    "    print(\"%s: %s\" % (w, porter.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# улучшенный вараинт стеммера Портера, умеет работать не только с английским текстом\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# смотрим список языков\n",
    "SnowballStemmer.languages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "будто: будт\n",
      "в: в\n",
      "вздев: вздев\n",
      "взревет: взревет\n",
      "влево: влев\n",
      "вой: во\n",
      "впился: впил\n",
      "вправо: вправ\n",
      "довелось: довел\n",
      "ему: ем\n",
      "и: и\n",
      "играя: игр\n",
      "к: к\n",
      "как: как\n",
      "конца: конц\n",
      "ль: л\n",
      "любовь: любов\n",
      "медноголосина: медноголосин\n",
      "меду: мед\n",
      "миноносина: миноносин\n",
      "миноносином: миноносин\n",
      "миноносица: миноносиц\n",
      "миноносочка: миноносочк\n",
      "миноносочки: миноносочк\n",
      "миноносцем: миноносц\n",
      "миноносцу: миноносц\n",
      "миноносьему: минонос\n",
      "мир: мир\n",
      "морям: мор\n",
      "морями: мор\n"
     ]
    }
   ],
   "source": [
    "ruswords = set(word_tokenize(text))\n",
    "\n",
    "for w in sorted(ruswords)[20:50]:\n",
    "    print(\"%s: %s\" % (w, snowball.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация и POS-tagging\n",
    "\n",
    "Прямо скажем, это не самая сильная сторона NLTK.  Для этих задач лучше использовать `pymorphy2` и `pymystem3` для русского языка и `Spacy` для европейских.\n",
    "\n",
    "### Лемматизация\n",
    "\n",
    "**Лемматиза́ция** — процесс приведения словоформы к лемме, т.е. нормальной (словарной) форме. Это более сложная задача, чем стемминг, но и результаты дает гораздо более осмысленные, особенно для языков с богатой морфологией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для большинства слов возможно несколько разборов (т.е. несколько разных лемм, несколько разных частей речи и т.п.). Теггер генерирует  все варианты, ранжирует их по вероятности и по умолчанию выдает наиболее вероятный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# на вход принимает по одному слову!\n",
    "wnl.lemmatize('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить другие разборы тоже можно, но способ зависит от конкретной программы/библиотеки. Например, `WordNetLemmatizer'у` можно указать часть речи разбираемого слово, и тогда он будет ориентироваться на нее при выборе леммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('running', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без указания частей речи WNL будет работать скорее как стеммер, так что не удивляйтесь, увидев что-то вроде \"wa\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('was'))\n",
    "print(wnl.lemmatize('was', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'cat', 'like', 'rat']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# как лемматизировать несколько слов\n",
    "[wnl.lemmatize(w) for w in word_tokenize('my cat likes rats')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-tagging\n",
    "\n",
    "**Частеречная разметка**, или **POS-tagging** _(part of speech tagging)_ —  определение части речи и грамматических характеристик слов в тексте (корпусе) с приписыванием им соответствующих тегов.\n",
    "\n",
    "### UPenn Tagset\n",
    "\n",
    "Теггер в NLTK по умолчанию использует своеобразную систему тегов (очень и очень устаревшую), которая, в числе прочего, описана [вот здесь](https://www.nltk.org/book/ch05.html). Она называется _UPenn Tagset_ — по английскому корпусу Penn Treebank, где она использовалась.\n",
    "\n",
    "* CC — coordinating conjunction\n",
    "* CD — cardinal digit\n",
    "* DT — determiner\n",
    "* EX — existential there (“there is”, “there exists”)\n",
    "* FW — foreign word\n",
    "* IN — preposition/subordinating conjunction\n",
    "* JJ — adjective (‘big’)\n",
    "* JJR — adjective, comparative (‘bigger’)\n",
    "* JJS — adjective, superlative (‘biggest’)\n",
    "* LS — list marker\n",
    "* MD — modal ('could', 'will')\n",
    "* NN — noun, singular (‘desk’)\n",
    "* NNS — noun plural (‘desks’)\n",
    "* NNP — proper noun, singular (‘Harrison’)\n",
    "* NNPS — proper noun, plural (‘Americans’)\n",
    "* PDT — predeterminer (‘all the kids’)\n",
    "* POS — possessive ending ('parent’s')\n",
    "* PRP — personal pronoun ('I', 'he', 'she')\n",
    "* PRPS — possessive pronoun ('my', 'his', 'hers')\n",
    "* RB — adverb ('very', 'silently')\n",
    "* RBR — adverb, comparative ('better')\n",
    "* RBS — adverb, superlative ('best')\n",
    "* RP — particle ('give up')\n",
    "* TO — to-particle ('to go') \n",
    "* UH — interjection ('errrrrrrrm')\n",
    "* VB — verb, base form ('take')\n",
    "* VBD — verb, past tense ('took')\n",
    "* VBG — verb, gerund/present participle ('taking')\n",
    "* VBN — verb, past participle ('taken')\n",
    "* VBP — verb, sing. present, non-3d ('take')\n",
    "* VBZ — verb, 3rd person sing. present ('takes')\n",
    "* WDT — wh-determiner ('which')\n",
    "* WP — wh-pronoun ('who', 'what')\n",
    "* WP — possessive wh-pronoun ('whose')\n",
    "* WRB — wh-abverb ('where', 'when')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'NNP'),\n",
       " ('there', 'EX'),\n",
       " ('I', 'PRP'),\n",
       " (\"'m\", 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('POS-tagger', 'NN')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"Hey there I'm a POS-tagger\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Dependencies\n",
    "\n",
    "Но — ура! — теперь подерживает и тегсет из _Universal Dependencies_, и **лучше использовать его**. Это нужно указать с помощью специального параметра `tagset` (предварительно скачав). Подробнее про проект можно почитать [вот тут](http://universaldependencies.org/), а про теги — [вот тут](http://universaldependencies.org/u/pos/). Вот список основных тегов UD:\n",
    "\n",
    "* ADJ: adjective\n",
    "* ADP: adposition\n",
    "* ADV: adverb\n",
    "* AUX: auxiliary\n",
    "* CCONJ: coordinating conjunction\n",
    "* DET: determiner\n",
    "* INTJ: interjection\n",
    "* NOUN: noun\n",
    "* NUM: numeral\n",
    "* PART: particle\n",
    "* PRON: pronoun\n",
    "* PROPN: proper noun\n",
    "* PUNCT: punctuation\n",
    "* SCONJ: subordinating conjunction\n",
    "* SYM: symbol\n",
    "* VERB: verb\n",
    "* X: other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'NOUN'),\n",
       " ('there', 'DET'),\n",
       " ('I', 'PRON'),\n",
       " (\"'m\", 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('POS-tagger', 'NOUN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"Hey there I'm a POS-tagger\"), tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "1. Скачайте файл с английским текстом \"Алисы в Зазеркалье\" из папки с семинаром.\n",
    "2. Очистите его от пунктуации и приведите к нижнему регистру\n",
    "3. Разбейте на биграммы и на триграммы с помощью NLTK. Выведите по 20 самых частотных биграмм и триграмм.\n",
    "4. Сделайте частеречную разметку текста и запишите в новый файл так, чтобы на каждой строке было одно слово и его тег.\n",
    "4. Лемматизируйте текст и запишите в новый файл.\n",
    "5. Очистите лемматизированный текст от стоп-слов и составьте частотный список слов. Выведите 30 самых частотных.\n",
    "6. Посчитайте ipm слов 'alice', 'unicorn', 'tweedledum', 'tweedledee', 'walrus', 'looking-glass'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная инфомация\n",
    "### Достаем части речи из WordNet\n",
    "\n",
    "Выше уже было показано, что для улучшения качества лемматизации нужно знать часть речи. Можно достать ее из корпуса WordNet, который и лежит в основе описанного выше лемматизатора. [Напишем](https://rustyonrampage.github.io/text-mining/2017/11/23/stemming-and-lemmatization-with-python-and-nltk.html) функцию, которая будет извлекать из ворднета часть речи для заданного слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_pos(word):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len([item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len([item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len([item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len([item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    \n",
    "    # first indexer for getting the top POS from list, \n",
    "    # second indexer for getting POS from tuple (POS: count)\n",
    "    return most_common_pos_list[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "I: I\n",
      "was: be\n",
      "shocked: shock\n",
      "but: but\n",
      "it: it\n",
      "worked: work\n"
     ]
    }
   ],
   "source": [
    "# пример попроще \n",
    "print(wnl.lemmatize('were', get_pos('were')))\n",
    "\n",
    "# пример посложнее\n",
    "text = \"I was shocked but it worked\"\n",
    "for w in word_tokenize(text):\n",
    "    print('%s: %s' % (w, wnl.lemmatize(w, get_pos(w))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация документов\n",
    "\n",
    "Умеет ли NLTK что-то посложнее выделения N-грамм и стемминга? Да! Посмотрим пример классификации документов, т.е. разбиение некоторого корпуса текстов на классы на основе различных признаков (`features`, в простонародье \"фичи\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем один из готовых корпусов: отзывы на фильмы\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]   # как называется вся эта конструкция в [ ]?\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно выбрать фичи, т.е. решить, на основании чего мы будем классифицировать тексты. Это называется красивым термином `feature engineering`. Например, мы можем выбрать в качестве фичей сами слова, т.е. наш классификатор будет принимать решение о том, к какому классу отнести тот или иной текст, на основании слов, которые в нем встречаются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# смотрим частотное распределение слов\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "\n",
    "# обрезаем низкочастотный хвост: берем только 2000 самых частотных слов из всех\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "# функция, которая извлекает фичи из документа\n",
    "def document_features(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно создать и обучить **классификатор** (для примера возьмем наивный байесовский классификатор). Подробнее о классификации, кластеризации и прочих увлекательных вещах можно узнать, углубившись в тему \"машинное обучение\" (осторожно: математика!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сопоставляем фичи и классы\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# разбиваем корпус на тренировочную и тестовую выборку\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "\n",
    "# обучаем классификатор\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь классифицируем тестовую выборку и оцениваем качество работы классификатора с помощью подсчета **accuracy** (\"точность\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "Most Informative Features\n",
      " contains(unimaginative) = True              neg : pos    =      8.4 : 1.0\n",
      "        contains(welles) = True              neg : pos    =      7.7 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      7.0 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      6.8 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      6.6 : 1.0\n",
      "       contains(jumbled) = True              neg : pos    =      6.4 : 1.0\n",
      "        contains(shoddy) = True              neg : pos    =      6.4 : 1.0\n",
      "       contains(singers) = True              pos : neg    =      6.3 : 1.0\n",
      "           contains(ugh) = True              neg : pos    =      5.8 : 1.0\n",
      "         contains(waste) = True              neg : pos    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# оцениваем качество\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# смотрим 10 самых информативных фичей\n",
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
