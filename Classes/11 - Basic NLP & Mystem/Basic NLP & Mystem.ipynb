{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основы NLP\n",
    "\n",
    "Ура, наконец-то мы добрались до анализа текста! Компьютерные методы анализа текстов на естественном языке (русском, английском, китайском и т.д.) по-английски называются **Natural Language Processing**, сокращенно — **NLP** (не путать с нейролингвистическим программированием, которое вовсе не про комьютеры, а про психологию!) Эту аббревиатуру вы будете встречать очень часто наряду с русским термином **компьютерная лингвистика**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-граммы\n",
    "\n",
    "Самые основы анализа естественного языка — это создание частотных списков и выделение N-грамм. **N-граммы** — это сочетания из N элементов (слов, символов), идущих друг за другом. Одиночные элементы называются униграммами, сочетания из двух элементов — биграммами, из трёх — триграммами, а дальше все пишется цифрами: 4-граммы, 5-граммы и т.д. Биграммы мы уже считали, [когда проходили циклы](https://github.com/ancatmara/python-for-dh/blob/master/Classes/6/Loops%20%26%20Iteration.ipynb).\n",
    "\n",
    "### Частотность\n",
    "Многие компьтерные методы анализа текста основаны на статистике — в нашем случае это частотность символов / словоформ/ слов / биграмм / триграмм и т.д., ее отношение к длине текста, средняя длина текстов и прочие цифры. :) \n",
    "\n",
    "Мы начнем с самого простого — с подсчета **частотности** слов. Зачем нам ее знать? Например, она говорит о том, какие слова наиболее характеры для того или иного текста. Сравнивая частотные слова в разных текстах можно определить степень их близости, а также выявить явления, характерные для языка в целом. Кроме того, частотные слова могут помочь нам сгруппировать (это еще называется красивым словом **кластеризация**) тексты по жанру, стилю, эпохе. Как вы думаете, почему?\n",
    "\n",
    "**Абсолютная частота слова** — это количество употреблений слова в тексте. Но это не очень удобная штука, ведь тексты различаются по длине и тематике. Например, если мы захотим узнать, кто популярнее: котики или пёсики, и возьмем для анализа один длинный текст про котиков и один короткий текст про пёсиков, то слово \"котик\", вероятно, окажется частотнее. А если наоборот — то частотнее будет \"пёсик\", и это никак не поможет нам ответить на поставленный вопрос, или — что ещё хуже — приведёт к ложным выводам.\n",
    "\n",
    "**Относительная частота слова** — это отношение его абсолютной частоты к какой-нибудь другой величине, например, к длине текста. Существуют разные способы подсчета относительной частоты, но мы будем использовать **ipm** *(items per million).* Как следует из названия, это отношение абсолютной частоты какого-либо элемента к объему корпуса, умноженное на миллион. Это можно записать в виде формулы:\n",
    "\n",
    "$$ ipm_{word} = \\dfrac{f_{word}}V_{corpus} \\        \\times \\  1,000,000 $$ \n",
    "\n",
    "Например, если текст состоит из 500 слов, и слово \"котик\" встречается там 50 раз, то \n",
    "\n",
    "$$ ipm_{kotik} = \\dfrac{50}{500} \\       \\times \\  1,000,000 \\     = 100,000 $$ \n",
    "\n",
    "\n",
    "Чем больше относительная частота слова, тем оно важнее (например, для определения темы текста).\n",
    "\n",
    "### Закон Ципфа\n",
    "\n",
    "**Закон Ципфа** («ранг—частота») — эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (т.н. рангу этого слова). Например, второе по используемости слово встречается примерно в два раза реже, чем первое, третье — в три раза реже, чем первое, и так далее. Выглядит это распределение вот так.\n",
    "\n",
    "\n",
    "![](https://i.pics.livejournal.com/eponim2008/17443609/234916/234916_original.jpg)\n",
    "\n",
    "Закон назван именем американского лингвиста Джорджа Ципфа (правда, популяризировал он данную закономерность не для лингвистических данных, а для описания распределения экономических сил и социального статуса). Если закон Ципфа соблюдается — значит, перед вами нормальный текст на естественном языке. Если нет, то что-то с ним не так... \n",
    "\n",
    "**Частотный словарь русского языка**, составленный на основе [НКРЯ](http://ruscorpora.ru/search-main.html) О.Н. Ляшевской и С.А. Шаровым, можно найти [вот тут](http://dict.ruslang.ru/freq.php).\n",
    "\n",
    "\n",
    "### Стоп-слова\n",
    "\n",
    "Частотный список любого языка будут возглавлять служебные слова (частицы, предлоги, союзы) и неполнозначные глаголы (\"говорить\", \"сказать\", \"(с)делать\" и т.п.), это лингвистическая универсалия. Как правило, они ничего не дают нам для анализа конкретного текста, поэтому от них часто избавляются перед тем, как начать подсчеты. В инструментах для автоматического анализа текста часто есть встроенные списки стоп-слов, но об этом мы поговорим на следующем семинаре. Самостоятельно составить список стоп-слов можно, например, взяв верхушку частотного словаря. Сколько первых слов выкидывать — зависит от исследовательской задачи и вашей интуиции. Кстати, часто отрезают не только топ частотника, но и длинный низкочастотный хвост, поскольку слова, которые встречаются в корпусе один-два раза, тоже не особенно информативны. \n",
    "\n",
    "Для анализа русской художественной литературы я когда-то делала [вот такой список](https://www.dropbox.com/s/onmfg7gztfddyxl/rus_stopwords.txt?dl=0), можете пользоваться. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem\n",
    "\n",
    "**Mystem** — это свободно распространяемый морфологический анализатор для русского языка с закрытым исходным кодом. То есть мы можем его бесплатно скачать с сайта и пользоваться им, но не можем посмотреть, что у него внутри и как оно работает. Mystem был придуман одним из создателей Яндекса Ильёй Сегаловичем. Некоторый потомок mystem'а до сих пор работает внутри большого поисковика Яндекса, анализируя слова при поиске.\n",
    "\n",
    "Mystem значит my stemmer, **стемминг** — это разбиение формы на основу и флексию. Но на самом деле Mystem может гораздо больше: устанавливать словарную форму слова (или **лемму**, как говорят лингвисты), определять часть речи и грамматическую форму слова. В последних версиях Mystem умеет и выбирать из нескольких возможных грамматических разборов один, наиболее верный. Но не нужно забывать, что программа, как и человек, может ошибаться. :)\n",
    "\n",
    "У Mystem нет графического интерфейса (GUI), и запустить его можно только из командной строки. Но это не повод паниковать, потому что для него написана питоновская обертка, `pymystem3`. Этой библиотеки нет ни в стандартной сборке, ни даже в анаконде, поэтому ее нужно специально установить. Для этого все же придется воспользоваться командной строкой, набрав в ней одну из двух команд (№2 — только для обладателей анаконды)\n",
    "\n",
    "    pip install pymystem3\n",
    "    conda install pymystem3\n",
    "Исходники `pymystem3` и вся документаци я лежат [вот тут](https://github.com/Digsolab/pymystem3). Это проще и удобнее, потому что с тем, что выдаёт `pymystem3`, можно сразу работать как с питоновскими структурами данных. Но медленнее. Иногда гораздо-гораздо медленнее, чем разметить один файл mystem'ом сразу. Так что если вы все же хотите научиться запускать обычный майстем из командной строки или с помощью питона, но при этом в консоли, то посмотрите [вот этот семинар](https://github.com/ancatmara/learnpython2017/blob/master/%D0%A1%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80%D1%8B/3.%20Mystem.md).\n",
    "\n",
    "А еще можно пользоваться Mystem и другими морфологическими анализаторами [вот на этом сайте](http://web-corpora.net/wsgi/mystemplus.wsgi/mystemplus/).\n",
    "\n",
    "### Лемматизация\n",
    "\n",
    "**Лемматизация** — это определение леммы для каждого слова, иными словами — приведение слова к словарной форме. На данном этапе из всех возможностей Mystem нам понадобится только она. С помощью `pymystem3` все делается очень просто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "на бледно-голубой эмаль,\n",
      "какой мыслимый в апрель,\n",
      "береза ветвь подымать\n",
      "и незаметно вечерель\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# не забудьте сначала установить библиотеку pymystem3\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "text = \"\"\"На бледно-голубой эмали,\n",
    "Какая мыслима в апреле,\n",
    "Березы ветви поднимали\n",
    "И незаметно вечерели\"\"\"\n",
    "\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "\n",
    "print(type(lemmas)) # функция возвращает список\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "1. Скачать [текст \"Капитанской дочки\"](https://www.dropbox.com/s/aky2md6724r3yww/%D0%BA%D0%B0%D0%BF%D0%B8%D1%82%D0%B0%D0%BD%D1%81%D0%BA%D0%B0%D1%8F%20%D0%B4%D0%BE%D1%87%D0%BA%D0%B0.txt?dl=0) или любого другого большого произведения на русском языке, сохранить в файле формата .txt и затем в программе прочитать текст из файла. Очистить текст от пунктуации. Обратите внимание, что типичная для русского языка пунктуация вроде кавычек-ёлочек и длинных тире должна быть удалена, а дефисы между словами должны остаться. Подсказка: первое действие — удалить всю \"приклеенную\" к словам пункуацию, второе действие — удалить тире.\n",
    "2. Лемматизировать все слова в тексте с помощью `pymystem3`.\n",
    "3. Создать частотный список лемм в тексте. Совпадают ли первые 50-100 слов в нем с верхушкой [частотного словаря НКРЯ](http://dict.ruslang.ru/freq.php?act=show&dic=freq_freq&title=%D7%E0%F1%F2%EE%F2%ED%FB%E9%20%F1%EF%E8%F1%EE%EA%20%EB%E5%EC%EC) в целом? А если посмотреть [подкорпус художественной литературы](http://dict.ruslang.ru/freq.php?act=show&dic=freq_fiction&title=%D7%E0%F1%F2%EE%F2%ED%FB%E9%20%F1%EB%EE%E2%E0%F0%FC%20%F5%F3%E4%EE%E6%E5%F1%F2%E2%E5%ED%ED%EE%E9%20%EB%E8%F2%E5%F0%E0%F2%F3%F0%FB)?\n",
    "4. Избавиться от стоп-слов и снова посмотреть частотный список. Напечатать 100 самых частотных слов (слово и его частота). Говорят ли они нам что-нибудь о тексте? Можем ли мы определить тему текста, не читая его? А стиль? Или что-нибудь еще?\n",
    "5. Разбить текст на биграммы, создать частотный список биграмм, напечатать первые 100. Дают ли они нам какую-нибудь дополнительную информацию по сравнению с униграммами? Заметили ли вы в них что-нибудь странное? Почему так получилось?\n",
    "6. Посчитать количество слов (униграмм) в тексте. Посчитать, сколько слов имеют частоту < 6 и какой это процент от общего числа слов. Напечатать 10 слов с частотой 1, 10 слов с частотой 2 — и так до частоты 5, а затем выбросить все низкочастотные слова (с частотой меньше 6). Сохранить получившийся после всех этих манипуляций (лемматизация + удаление стоп-слов + удаление низкочастотного хвоста) в новый файл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как вывести N самых частотных слов?\n",
    "\n",
    "Отсортированный по частотности список слов можно получить несколькими способами. \n",
    "\n",
    "### Способ №1: `Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'я': 9, 'и': 8, 'свойства': 4, 'умные': 4, 'что': 3, 'мира': 3, 'частей': 3, 'мир': 2, 'видел': 2, 'части': 2, 'есть': 2, 'не': 2, 'в': 2, 'их': 2, 'говорил': 1, 'себе': 1, 'вижу': 1, 'но': 1, 'весь': 1, 'недоступен': 1, 'моему': 1, 'взгляду': 1, 'только': 1, 'все': 1, 'называл': 1, 'частями': 1, 'наблюдал': 1, 'этих': 1, 'наблюдая': 1, 'делал': 1, 'науку': 1, 'понимал': 1, 'тех': 1, 'же': 1, 'частях': 1, 'делил': 1, 'давал': 1, 'им': 1, 'имена': 1, 'зависимости': 1, 'от': 1, 'свойств': 1, 'были': 1})\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "я говорил себе что я вижу мир но весь мир недоступен моему взгляду и\n",
    "я видел только части мира и все что я видел я называл частями мира и я\n",
    "наблюдал свойства этих частей и наблюдая свойства частей я делал науку я\n",
    "понимал что есть умные свойства частей и есть не умные свойства в тех же\n",
    "частях я делил их и давал им имена и в зависимости от их свойств части\n",
    "мира были умные и не умные\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(text.split())\n",
    "\n",
    "print(counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('я', 9), ('и', 8), ('свойства', 4), ('умные', 4), ('что', 3)]\n"
     ]
    }
   ],
   "source": [
    "# как напечатать первые N элементов \n",
    "print(counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Способ №2: `lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'я': 9, 'говорил': 1, 'себе': 1, 'что': 3, 'вижу': 1, 'мир': 2, 'но': 1, 'весь': 1, 'недоступен': 1, 'моему': 1, 'взгляду': 1, 'и': 8, 'видел': 2, 'только': 1, 'части': 2, 'мира': 3, 'все': 1, 'называл': 1, 'частями': 1, 'наблюдал': 1, 'свойства': 4, 'этих': 1, 'частей': 3, 'наблюдая': 1, 'делал': 1, 'науку': 1, 'понимал': 1, 'есть': 2, 'умные': 4, 'не': 2, 'в': 2, 'тех': 1, 'же': 1, 'частях': 1, 'делил': 1, 'их': 2, 'давал': 1, 'им': 1, 'имена': 1, 'зависимости': 1, 'от': 1, 'свойств': 1, 'были': 1}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "\n",
    "for word in text.split():\n",
    "    if word in counts:\n",
    "        counts[word] += 1\n",
    "    else:\n",
    "        counts[word] = 1\n",
    "\n",
    "# это неотсортированный словарь\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('я', 9), ('и', 8), ('свойства', 4), ('умные', 4), ('что', 3), ('мира', 3), ('частей', 3), ('мир', 2), ('видел', 2), ('части', 2), ('есть', 2), ('не', 2), ('в', 2), ('их', 2), ('говорил', 1), ('себе', 1), ('вижу', 1), ('но', 1), ('весь', 1), ('недоступен', 1), ('моему', 1), ('взгляду', 1), ('только', 1), ('все', 1), ('называл', 1), ('частями', 1), ('наблюдал', 1), ('этих', 1), ('наблюдая', 1), ('делал', 1), ('науку', 1), ('понимал', 1), ('тех', 1), ('же', 1), ('частях', 1), ('делил', 1), ('давал', 1), ('им', 1), ('имена', 1), ('зависимости', 1), ('от', 1), ('свойств', 1), ('были', 1)]\n"
     ]
    }
   ],
   "source": [
    "# чтобы его отсортировать, нужно написать следующее\n",
    "\n",
    "sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('я', 9), ('и', 8), ('свойства', 4), ('умные', 4), ('что', 3)]\n"
     ]
    }
   ],
   "source": [
    "# как напечатать первые N элементов \n",
    "print(sorted_counts[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
